---
title: "Data Science Applied to Ag - Final Project - ML"
format:
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Md Shakir Moazzem, Umar Munir
---

# Introduction  

This script contains the data wrangling and feature engineering steps for the final project.

# Start of data wrangling

## Loading packages  

The following code chunk will load necessary packages.

```{r Setup, message=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("readxl")
#install.packages("janitor")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("readr")
#install.packages("lubridata")
#install.packages("stringr")

# Loading packages 

library(tidyverse)
library(readxl) # to read excel files
library(janitor) # to clean data; helps fix and standardize the column names
library(dplyr) # wrangling
library(tidyr) # wrangling
library(readr) # to export csv
library(lubridate)
library(stringr)

```


## Reading "training" data  

The following code chunk will read the csv files for the 3 training data sets

```{r training data import, message=F, warning=F}

#reading the csv files for the 3 training data sets 

trait_df <- read_csv("../data/training/training_trait.csv") 
meta_df  <- read_csv("../data/training/training_meta.csv")
soil_df  <- read_csv("../data/training/training_soil.csv")

submission_test <- read_csv("../data/testing/testing_submission.csv")
soil_test <- read_csv("../data/testing/testing_soil.csv")
meta_test <- read_csv("../data/testing/testing_meta.csv")


```



## Data wrangling on "training" data

The code below creates a function to adjust the yield_mg_ha for 15.5% grain moisture.

```{r}

# Function to transform yield to 15.5% moisture
adjust_yield <- function(yield_mg_ha, grain_moisture) {
  yield_mg_ha * (100 - grain_moisture) / (100 - 15.5)
}

```


The code below conducts data wrangling on "trait_df"

```{r clean_and_summarize_trait, message=F, warning=F}

trait_clean <- trait_df %>%
  select(-block) %>%
  mutate(
    site = str_remove_all(site, "[a-z]"), # removing all lowercase letters from site
    site = str_replace(site, "-.*$", ""), # keeping only text before any dash
    site = str_replace(site, "_.*$", "") # keeping only text before any underscore
  ) %>%
  group_by(year, site, hybrid) %>%
  summarize(
    yield_mg_ha    = mean(yield_mg_ha, na.rm = TRUE),
    grain_moisture = mean(grain_moisture, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    adjusted_yield = adjust_yield(yield_mg_ha, grain_moisture),
  ) %>%
  select(-yield_mg_ha, -grain_moisture) %>%
  ungroup()


trait_clean

```

```{r}

submission_test_clean <- submission_test %>%
  mutate(
    site = str_remove_all(site, "[a-z]"), # removing all lowercase letters from site
    site = str_replace(site, "-.*$", ""), # keeping only text before any dash
    site = str_replace(site, "_.*$", "") # keeping only text before any underscore
  ) %>%
  group_by(year, site, hybrid) %>%
  select(-yield_mg_ha) %>%
  ungroup()

submission_test_clean

```


The following code chunk will conduct data wrangling on  "training_soil.csv"


```{r data_wrangling_training_soil, message=F, warning=F}

soil_clean <- soil_df %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  ungroup()

soil_clean
```


```{r}

soil_test_clean <- soil_test %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  ungroup()

soil_test_clean

```


The following code chunk will conduct data wrangling on  "training_meta.csv".

```{r clean_meta_sites}

meta_clean <- meta_df %>%
  rename(
    lon = longitude,
    lat = latitude
  ) %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  distinct(year, site, .keep_all = TRUE) %>%
  select(-previous_crop) %>%
  clean_names() %>%
  ungroup() 
  

meta_clean

```

```{r}

meta_test_clean <- meta_test %>%
  rename(
    lon = longitude,
    lat = latitude
  ) %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  distinct(year, site, .keep_all = TRUE) %>%
  select(-previous_crop) %>%
  clean_names() %>%
  ungroup() 
  

meta_test_clean

```



## EDA for cleaned data


```{r}
summary(trait_clean)

summary(soil_clean)

summary(meta_clean)

summary(submission_test_clean)

summary(soil_test_clean)

summary(meta_test_clean)

```



## Merging all 3 cleaned data frames for training data

```{r}

merged_train <- trait_clean %>%
  left_join(soil_clean, by = c("year","site")) %>%
  left_join(meta_clean, by = c("year","site"))


merged_test <- submission_test_clean %>%
  left_join(soil_test_clean, by = c("year","site")) %>%
  left_join(meta_test_clean, by = c("year","site"))


merged_all <- bind_rows(merged_train, merged_test)

merged_all

```
## EDA for cleaned merged data

```{r}
summary(merged_all)
```


# Open Source Daymet weather data download

The following code chunk will load necessary packages.  

```{r Setup, message=F, warning=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("sf") #to manipulate vector geospatial files
#install.packages("daymetr") #to retrieve data from daymet database website through R
#install.packages("remotes") #to install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") 
#remotes::install_github("ropensci/USAboundariesData")

# Loading packages

library(tidyverse) #need to load "tidyverse" package at first
library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr) #to retrieve data from daymet database website through R
library(remotes)
library(USAboundaries) # for US state boundaries
library(USAboundariesData)

```


The following code chunk will create a map of the USA and plot the sites in the map based on their latitude and longitude.


```{r create map of USA and add points, message=F, warning=F}

states <- us_states() %>% 
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #to remove "PR" (Puerto Rico), "AK" (Alaska), and "HI" (Hawaii)
  
ggplot() +
  geom_sf(data = states) + #"geom_sf()" is used to plot "sf" object, which we just created above as "states" object; plots all states and territories of USA
  geom_point(data = merged_all,
             aes(x = lon, #"Longitude" goes on longitude
                 y = lat) #"Latitude" goes on latitude
             ) +
  labs(
    title = "Corn Trial Site Locations (2014–2023)",
    x     = "Longitude",
    y     = "Latitude"
  )

```

The following code chunk will keep the observations for site-years having latitude and longitude withing the Daymet range of co-ordinates.

Declaration of AI use: the following code chunk was inspired and subsequently modified on the basis of code initially generated by ChatGPT


```{r}

# Define Daymet bounding box (WGS-84)
min_lat <- 14.53
max_lat <- 52.00
min_lon <- -131.104
max_lon <- -52.95

# Filter merged to Daymet’s valid range, dropping any NA coords
merged_daymet <- merged_all %>%
  filter(
    !is.na(lat),
    !is.na(lon),
    lat  >= min_lat,
    lat  <= max_lat,
    lon  >= min_lon,
    lon  <= max_lon
  )

# Report how many rows remain (and were dropped)
message("Rows kept: ", nrow(merged_daymet), 
        " (dropped: ", nrow(merged_all) - nrow(merged_daymet), ")")


```
The following code chunk will extract unique combinations of year, site, and their coordinates.

```{r unique_site_years_with_coords}

site_year_df <- merged_daymet %>%
  select(year, site, lon, lat) %>%  # need to include longitude and latitude along with site-years
  distinct() %>%                    
  arrange(year, site)

site_year_df

```

The following code chunk will download the weather data for all unique combinations of year, site, and their coordinates in the "site_year_df" object.

```{r}

weather_daymet_all <- site_year_df %>% 
  mutate(weather = pmap(list(.y = year, 
                             .site = site, 
                             .lat = lat, 
                             .lon = lon), 
                        function(.y, .site, .lat, .lon) 
                          download_daymet( 
                            site = .site, 
                            lat = .lat, #specifying ".lat" placeholder for "lat = " argument
                            lon = .lon, #specifying ".lon" placeholder for "lon = " argument
                            start = .y, 
                            end = .y, 
                            simplify = T,
                            silent = T) %>% #end of " download_daymet()" function
                          rename(.year = year,
                                 .site = site) 
                        )) 


weather_daymet_all



```


```{r}

daymet_all_unnest <- weather_daymet_all %>%
  unnest(weather) %>% 
  pivot_wider(names_from = measurement, 
              values_from = value) %>% 
  janitor::clean_names() 

daymet_all_unnest




```



```{r}
write_csv(daymet_all_unnest,
          "../data/fieldweatherdata.csv"
          )
```


```{r}
fieldweather <- read_csv("../data/fieldweatherdata.csv")

fieldweather

```




# Feature engineering

## Loading packages

```{r}

#install.packages("ggridges")

library(ggridges)
library(tidyverse)

```


The following code chunk will keep desired variables that we will use further and get abbreviated month name based on the date.


```{r fe_month}

fe_month <- fieldweather %>%
  # Selecting needed variables
  dplyr::select(year, site, lat, lon,
                #strength_gtex,
                yday,
                dayl.s = dayl_s, #to rename variable name from "dayl_s" to dayl.s
                prcp.mm = prcp_mm_day, #to rename variable name to "prcp.mm"
                srad.wm2 = srad_w_m_2,#to rename variable name to "srad.wm2"
                tmax.c = tmax_deg_c, #to rename variable name to "tmax.c"
                tmin.c = tmin_deg_c,#to rename variable name to "tmin.c"
                vp.pa = vp_pa #to rename variable name to "vp.pa"
                ) %>%
  # Creating a date class variable  
  mutate(date_chr = paste0(year, "/", yday)) %>% #The format of "date_chr" is "chr", we will change it to "date" format when we use "as.Date()" function in the next line
  mutate(date = as.Date(date_chr, "%Y/%j")) %>% #"%Y/%j" because we used "/" in paste0(year, "/", yday) at the previous line #If instead of using "/" we used "-" in the previous line as "mutate(date_chr = paste0(year, "-", yday))", we would simply use "mutate(date = as.Date(date_chr, "%Y-%j"))" in the current line 
  #Date-related documentation details: https://www.r-bloggers.com/2013/08/date-formats-in-r/
  # Extracting month from date  
  mutate(month = month(date)) %>% #we use month() function to extract only the numerical number of the month as "dbl" format (number of the month e.g., "1" for "January" etc) from an object that has the year/month/date data (i.e., "date" object that we created in the previous line) #the 1st "month" inside mutate is to name the new column, the 2nd "month" after = sign is the month() function from lubridate package #month is in "dbl" (double) format
  mutate(month_abb = month(date, label = T)) #To get abbreviated month name e.g., Jan, Feb, Mar,...,Dec #month_abb is in "ord" (ordinal) format


fe_month

```


Now, let's summarize daily weather variables based on month.  

```{r fe_month_sum}

fe_month_sum <- fe_month %>%
  group_by(year, site, month_abb) %>% #If we do a summarise() after group_by(), any column that's not in the group_by() is gone, so we need to include "strength_gtex" in the group_by() to include it in the data frame because "strength_gtex" is our response variable so we must keep it in the data frame 
  #Because we are gonna be applying a "summarize()" function to different columns, we are gonna use a function called "across()" 
  summarise(across(.cols = c(dayl.s,
                             srad.wm2,
                             tmax.c,
                             tmin.c,
                             vp.pa),
                   .fns = mean, #do not indicate the actual function "mean()", just use the word "mean"
                   .names = "mean_{.col}"), #specifying the weather variables that we want their mean as new column variables #1st across() is applying the "mean" function (to summarize "mean")
            across(.cols = prcp.mm,
                   .fns = sum,
                   .names = "sum_{.col}"
                   ) #specifying the weather variable (prcp.mm) that we want its sum as new column variable #2nd across() is summarizing sum #2nd across() is applying the "sum" function (to summarize "sum")
            ) %>%
  ungroup() #To convert from "group" to "tibble"


fe_month_sum

#View(fe_month_sum)

```


Let's check tmax.c and prcp.mm for the first site-year and month. 

[Note: the following code chunk is for double checking to make sure that we did everything okay the way we intended] 

```{r}

fe_month %>%
  filter(year == 2014 & 
           site == "DEH1" &
           month_abb == "Jan") %>%
  summarise(tmax.c = mean(tmax.c),
            prcp.mm = sum(prcp.mm))


```

The code below will put the month as part of the column name. 


```{r fe_month_sum_wide}

fe_month_sum_wide <- fe_month_sum %>%
  pivot_longer(cols = mean_dayl.s:sum_prcp.mm) %>% 
  mutate(varname = paste0(name, "_", month_abb)) %>% 
  dplyr::select(-name, -month_abb) %>% 
  pivot_wider(names_from = varname,
              values_from = value) %>%
  # Rounding to one decimal point
  mutate(across(c(4:9), ~round(., 1) )) 

View(fe_month_sum_wide)  

```

```{r}

write_csv(fe_month_sum_wide,
           "../data/fe_month_sum_wide.csv")

```


```{r}


merged_full <- merged_all %>%
  left_join(fe_month_sum_wide, by = c("year","site"))

View(merged_full)


```


The following code chunk will export the feature engineered data file as .csv in the local disk, which we will use in our further data analysis.

```{r}

write_csv(merged_full,
           "../data/weather_monthsum.csv")

```

